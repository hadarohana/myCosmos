"""
Embeddings
"""
import pymongo
from pymongo import MongoClient
import os
import logging
logging.basicConfig(format='%(levelname)s :: %(asctime)s :: %(message)s', level=logging.DEBUG)
import time
import click
from joblib import Parallel, delayed
import pandas as pd
import re
import pickle
from collections import defaultdict
import math
from gensim.utils import simple_preprocess as tokenize
from gensim.models import Word2Vec, FastText # Create FastText model for more accuracy (but slower)
import numpy
import faiss

EMBEDDINGS_WORD2VEC_MODEL_FILE = "w2v_model"
db = None
num_processes =  None
index = None
word2int = None
int2word = None
xb = None

class SentencesIterator():
    def __init__(self, generator_function):
        self.generator_function = generator_function
        self.generator = self.generator_function()

    def __iter__(self):
        # reset the generator
        self.generator = self.generator_function()
        return self

    def __next__(self):
        result = next(self.generator)
        if result is None:
            raise StopIteration
        else:
            return result

def load_pages():
    """
    """
    current_docs = []
    for doc in db.objects.find().batch_size(num_processes):
        current_docs.append(tokenize(doc['content']))
        #print(current_docs)
        #print([i for doc in current_docs for i in doc])
        #print(len(current_docs))
        if len(current_docs) == num_processes:
            yield [i for doc in current_docs for i in doc]
            current_docs = []
    yield [i for doc in current_docs for i in doc]


def generate_model():
    global db
    logging.info('Generating gensim model for all documents')
    start_time = time.time()
    client = MongoClient(os.environ['DBCONNECT'])
    logging.info(f'Connected to client: {client}')
    db = client.pdfs
    sentences = SentencesIterator(load_pages)
    print('Calculating the embeddings...')
    model = Word2Vec(sentences, size=100, window=10, min_count=3, workers=4)
    print('Saving the model...')
    model.save(EMBEDDINGS_WORD2VEC_MODEL_FILE)
    print('WORD2VEC Model saved.')
    return model

def get_index():
    global index
    global word2int
    global xb
    global int2word
    #model = generate_model()
    model = Word2Vec.load(EMBEDDINGS_WORD2VEC_MODEL_FILE)
    #word2int={key:k for k,key in enumerate(model.wv.vocab.keys())}  
    int2word ={k:key for k,key in enumerate(model.wv.vocab.keys())}
    xb=numpy.array([model.wv[word] for word in model.wv.vocab.keys()])
    quantizer = faiss.IndexFlatIP(100) # Inner product cosine similarity
    nlist = 50 # Finetune this number of clusters
    m = 100 # bytes per vector
    #index = faiss.IndexIVFPQ(quantizer, 100, nlist, m, 8) # reduced accuray, fast
    index = faiss.IndexIVFFlat(quantizer, 100, nlist, faiss.METRIC_INNER_PRODUCT)
    #print(word2int)
    #print(xb)
    faiss.normalize_L2(xb)
    #print(xb.shape)
    index.train(xb)
    index.add(xb) 
    index.nprobe = 5
    # return index, word2int

def query(word, k):
    xq = xb[word2int[word]]
    print(xq)
    faiss.normalize_L2(xq)
    D, I = index.search(xq, k)
    return  [int2word[index] for index in I]

@click.command()
@click.argument('n_processes')
@click.argument('word')
@click.argument('k')
def click_wrapper(n_processes, word, k):
    global num_processes
    num_processes = int(n_processes)
    get_index()
    query(word, int(k))

if __name__ == '__main__':
    click_wrapper()


